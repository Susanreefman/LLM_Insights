{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05f1a3ac-cf0a-4fd7-b4b9-8350d2ad7247",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/lusfernandotorres/text-summarization-with-large-language-models/notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc4ac89-452b-4d23-a166-24a0ce8e6e4e",
   "metadata": {},
   "source": [
    "# Summarization with Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fa5344a4-8f31-45ba-be2e-d1c663c73eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "import torch\n",
    "import collections \n",
    "from collections import Counter\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "02231fb2-8a8f-4315-9769-90715787d4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/home/vboxuser/Documents/data.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5c050f8a-b623-4728-9c69-fd4e8f07755d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = tts(data['Text'],data['Summary'],test_size=0.1, shuffle=True, random_state=111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6e2d2dbb-1c19-46f3-9bbd-d76fa3913fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize function \n",
    "def tokenize(lines, token='word'):\n",
    "    \"\"\"Make from sentence a list like [\"Make\", \"from\", \"sentence\"]\"\"\"\n",
    "    assert token in ('word', 'char'), 'Unknown token type: ' + token\n",
    "    return [line.split() if token == 'word' else list(line) for line in lines]\n",
    "\n",
    "# padding function\n",
    "def truncate_pad(line, num_steps, padding_token):\n",
    "    if len(line) > num_steps:\n",
    "        return line[:num_steps]  # Truncate\n",
    "    return line + [padding_token] * (num_steps - len(line))  # Pad\n",
    "\n",
    "def build_array_sum(lines, vocab, num_steps):\n",
    "    \"\"\"fn to add eos and padding and also determine valid length of each data sample\"\"\"\n",
    "    lines = [vocab[l] for l in lines]\n",
    "    # Add end of sentence\n",
    "    lines = [l + [vocab['<eos>']] for l in lines]\n",
    "    #Create tensor with padding\n",
    "    array = torch.tensor([truncate_pad(l, num_steps, vocab['<pad>']) for l in lines])\n",
    "    #Make all lines equal length by padding\n",
    "    valid_len = (array != vocab['<pad>']).type(torch.int32).sum(1)\n",
    "    return array, valid_len\n",
    "\n",
    "# create the tensor dataset object \n",
    "def load_array(data_arrays, batch_size, is_train=True):\n",
    "    dataset = torch.utils.data.TensorDataset(*data_arrays)\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size, shuffle=is_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d1562a07-57a4-4718-a92b-b2cadbdcf83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_qkv(X, num_heads):\n",
    "    # Function to transpose the linearly transformed query key and values \n",
    "    X = X.reshape(X.shape[0], X.shape[1], num_heads, -1)\n",
    "    X = X.permute(0, 2, 1, 3)\n",
    "    return X.reshape(-1, X.shape[2], X.shape[3])\n",
    "\n",
    "def transpose_output(X, num_heads):\n",
    "    # For output formatting \n",
    "    X = X.reshape(-1, num_heads, X.shape[1], X.shape[2])\n",
    "    X = X.permute(0, 2, 1, 3)\n",
    "    return X.reshape(X.shape[0], X.shape[1], -1)\n",
    "    \n",
    "\n",
    "def sequence_mask(X, valid_len, value=0):\n",
    "    # Here masking is used so that irrelevant padding tokens are not considered while calculations\n",
    "    maxlen = X.size(1)\n",
    "    mask = torch.arange((maxlen), dtype=torch.float32)[None, :] < valid_len[:, None]    #device=X.device\n",
    "    X[~mask] = value\n",
    "    return X\n",
    "    \n",
    "\n",
    "def masked_softmax(X, valid_lens):\n",
    "    # the irrelevant tokens are given a very small negative value which gets ignored in the subsequent calculations\n",
    "    if valid_lens is None:\n",
    "        return nn.functional.softmax(X, dim=-1)\n",
    "    else:\n",
    "        shape = X.shape\n",
    "        if valid_lens.dim() == 1:\n",
    "            valid_lens = torch.repeat_interleave(valid_lens, shape[1])\n",
    "        else:\n",
    "            valid_lens = valid_lens.reshape(-1) \n",
    "        X = sequence_mask(X.reshape(-1, shape[-1]), valid_lens, value=-1e6)\n",
    "        return nn.functional.softmax(X.reshape(shape), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aabe0e70-0a28-41ff-8e05-439ca6183b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device(i=0):\n",
    "    if torch.cuda.device_count() >= i+1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "\n",
    "def grad_clipping(net, theta):\n",
    "    if isinstance(net, nn.Module):\n",
    "        params = [p for p in net.parameters() if p.requires_grad]\n",
    "    else:\n",
    "        params = net.params\n",
    "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4e7703-8a0d-45d9-80ec-3442930a5ae2",
   "metadata": {},
   "source": [
    "### Vocab\n",
    "#### Building vocabulary on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "496e2581-3b90-42c2-914a-214a33907f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the vocabulary class \n",
    "class Vocab:\n",
    "    def __init__(self, tokens=[], min_freq=0, reserved_tokens=[]):\n",
    "        # Flatten a 2D list if needed\n",
    "        if tokens and isinstance(tokens[0], list):\n",
    "            tokens = [token for line in tokens for token in line]\n",
    "        # Count token frequencies\n",
    "        counter = collections.Counter(tokens)\n",
    "        self.token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
    "                                  reverse=True)\n",
    "        # The list of unique tokens\n",
    "        self.idx_to_token = list(sorted(set(['<unk>'] + reserved_tokens + [\n",
    "            token for token, freq in self.token_freqs if freq >= min_freq])))\n",
    "        self.token_to_idx = {token: idx\n",
    "                             for idx, token in enumerate(self.idx_to_token)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if hasattr(indices, '__len__') and len(indices) > 1:\n",
    "            return [self.idx_to_token[int(index)] for index in indices]\n",
    "        return self.idx_to_token[indices]\n",
    "\n",
    "    def unk(self):  # Index for the unknown token\n",
    "        return self.token_to_idx['<unk>']\n",
    "    \n",
    "    def print_variable(self):\n",
    "        print(\"Variable idx_to_token:\", self.idx_to_token)\n",
    "        print(\"Variable idx_to_token:\", self.token_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8f85a4-f753-49be-b12c-2505bb36eaeb",
   "metadata": {},
   "source": [
    "### Multi Head Attention\n",
    "\n",
    "#### class EncoderBlock & Decoderblock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0ab7c4a6-5979-46c4-a68e-50dafd5fae00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main class \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, key_size, query_size, value_size, num_hiddens, num_heads, dropout, bias=False, **kwargs):\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = DotProductAttention(dropout)\n",
    "        self.w_q = nn.Linear(query_size, num_hiddens, bias=bias)\n",
    "        self.w_k = nn.Linear(key_size, num_hiddens, bias=bias)\n",
    "        self.w_v = nn.Linear(value_size, num_hiddens, bias=bias)\n",
    "        self.w_o = nn.Linear(num_hiddens, num_hiddens, bias=bias)\n",
    "        \n",
    "    def forward(self, queries, keys, values, valid_lens):\n",
    "        queries = transpose_qkv(self.w_q(queries), self.num_heads)\n",
    "        keys = transpose_qkv(self.w_k(keys), self.num_heads)\n",
    "        values = transpose_qkv(self.w_v(values), self.num_heads)\n",
    "        if valid_lens is not None:\n",
    "            valid_lens = torch.repeat_interleave(valid_lens, repeats = self.num_heads, dim=0)\n",
    "        output = self.attention(queries, keys, values, valid_lens)\n",
    "        output_concat = transpose_output(output, self.num_heads)\n",
    "        \n",
    "        return self.w_o(output_concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c952be-35f4-4a97-bbc3-d4752d10ffae",
   "metadata": {},
   "source": [
    "### Dot Product Attention \n",
    "A simplified form of attention that calculates the similar\\ity between query and key vectors by taking their dot product.\n",
    "#### (class MultiHeadAttention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a055dc27-9f1f-4a3b-a6ae-ab6388c4a270",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductAttention(nn.Module):\n",
    "    # The dot product attention scoring function \n",
    "    def __init__(self, dropout, **kwargs):\n",
    "        super(DotProductAttention, self).__init__(**kwargs)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, queries, keys, values, valid_lens=None):\n",
    "        d = queries.shape[-1]\n",
    "        scores = torch.bmm(queries, keys.transpose(1, 2))/math.sqrt(d)\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "        \n",
    "        return torch.bmm(self.dropout(self.attention_weights), values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d3a1a4-03b9-47f3-a3c0-e0a144460311",
   "metadata": {},
   "source": [
    "### Positionwise feedforward network \n",
    "#### (class Encoder & Decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "49f87d9c-b3f3-41a8-b929-cc9244d9e50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFFN(nn.Module):\n",
    "    def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_output, **kwargs):\n",
    "        super(PositionWiseFFN, self).__init__(**kwargs)\n",
    "        self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_output)\n",
    "    def forward(self, X):\n",
    "        return self.dense2(self.relu(self.dense1(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e067969-568b-48cc-bc7e-c0100313fe99",
   "metadata": {},
   "source": [
    "### Postional Encoding\n",
    "Provide positional information to the model, positional encoding is added to the input embeddings to convey information about the order of tokens in the sequence.\n",
    "#### (class TransformerEncoding TransformerDecoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2f274209-2d26-4d68-88a1-01ce13b17af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, num_hiddens, dropout, max_len=1000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
    "        X = torch.arange(max_len, dtype=torch.float32).reshape(-1, 1)/torch.pow(10000,torch.arange(0, num_hiddens,2, dtype=torch.float32)/num_hiddens)\n",
    "        self.P[:,:, 0::2] = torch.sin(X)\n",
    "        self.P[:, :, 1::2] = torch.cos(X)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = X + self.P[:, :X.shape[1], :].to(X.device)\n",
    "        return self.dropout(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098851d1-76c8-419e-a13c-7f3909bb77d2",
   "metadata": {},
   "source": [
    "### EncoderBlock & Transformer Encoder\n",
    "#### Initializing encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dd372d87-5aab-41dd-91d6-64712d3519bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for the block structure within \n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, \n",
    "                 ffn_num_hiddens, num_heads, dropout, use_bias=False, **kwargs):\n",
    "        super(EncoderBlock, self).__init__(**kwargs)\n",
    "        self.attention = MultiHeadAttention(key_size, query_size, value_size, num_hiddens,num_heads, dropout, use_bias)\n",
    "        self.addnorm1 = AddNorm(norm_shape, dropout)\n",
    "        self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens, num_hiddens)\n",
    "        self.addnorm2 = AddNorm(norm_shape, dropout)\n",
    "        \n",
    "    def forward(self, X, valid_lens):\n",
    "        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))\n",
    "        return self.addnorm2(Y, self.ffn(Y))\n",
    "\n",
    "# the main encoder class\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, use_bias=False, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__(**kwargs)\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "        self.pos_encoding = PositionalEncoding(num_hiddens, dropout)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module(\"block\"+str(i),EncoderBlock(key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, use_bias))\n",
    "    \n",
    "    def forward(self, X, valid_lens, *args):\n",
    "        X = self.pos_encoding(self.embedding(X)*math.sqrt(self.num_hiddens))\n",
    "        self.attention_weights = [None]*len(self.blks)\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X = blk(X, valid_lens)\n",
    "            self.attention_weights[i] = blk.attention.attention.attention_weights\n",
    "        return X\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d8ed33-958b-49fc-af9c-a925473036cb",
   "metadata": {},
   "source": [
    "### DecoderBlock & TransformerDecoder\n",
    "#### Initializing decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "62a49fad-ff86-43cd-a6c9-c63cf2fc3809",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, key_size, query_size, value_size, num_hiddens, norm_shape,\n",
    "                 ffn_num_input, ffn_num_hiddens, num_heads, dropout, i, **kwargs):\n",
    "        super(DecoderBlock, self).__init__(**kwargs)\n",
    "        self.i = i\n",
    "        self.attention1 = MultiHeadAttention(key_size, query_size, value_size, num_hiddens, num_heads, dropout)\n",
    "        self.addnorm1 = AddNorm(norm_shape, dropout)\n",
    "        self.attention2 = MultiHeadAttention(key_size, query_size, value_size, num_hiddens, num_heads, dropout)\n",
    "        self.addnorm2 = AddNorm(norm_shape, dropout)\n",
    "        self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens, num_hiddens)\n",
    "        self.addnorm3 = AddNorm(norm_shape, dropout)\n",
    "        \n",
    "    def forward(self, X, state):\n",
    "        enc_outputs, enc_valid_lens = state[0], state[1]\n",
    "        if state[2][self.i] is None: # true when training the model\n",
    "            key_values = X\n",
    "        else:                        # while decoding state[2][self.i] is decoded output of the ith block till the present time-step\n",
    "            key_values = torch.cat((state[2][self.i], X), axis=1)\n",
    "        state[2][self.i] = key_values\n",
    "        if self.training:\n",
    "            batch_size, num_steps, _ = X.shape\n",
    "            dec_valid_lens = torch.arange(1, num_steps+1, device = X.device).repeat(batch_size, 1)\n",
    "        else:\n",
    "            dec_valid_lens = None\n",
    "        X2 = self.attention1(X, key_values, key_values, dec_valid_lens)\n",
    "        Y = self.addnorm1(X, X2)\n",
    "        Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens)\n",
    "        Z = self.addnorm2(Y, Y2)\n",
    "        return self.addnorm3(Z, self.ffn(Z)), state\n",
    "\n",
    "# The main decoder class \n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, key_size, query_size, value_size, num_hiddens,\n",
    "                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, **kwargs):\n",
    "        super(TransformerDecoder, self).__init__(**kwargs)\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "        self.pos_encoding = PositionalEncoding(num_hiddens, dropout)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module(\"block\"+str(i), \n",
    "                                DecoderBlock(key_size, query_size, value_size,\n",
    "                                             num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, i))\n",
    "            self.dense = nn.Linear(num_hiddens, vocab_size)\n",
    "    \n",
    "    def init_state(self, enc_outputs, enc_valid_lens, *args):\n",
    "        return [enc_outputs, enc_valid_lens, [None]*self.num_layers]\n",
    "    \n",
    "    def forward(self, X, state):\n",
    "        X = self.pos_encoding(self.embedding(X)*math.sqrt(self.num_hiddens))\n",
    "        self._attention_weights = [[None]*len(self.blks) for _ in range(2)]\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X, state = blk(X, state)\n",
    "            self._attention_weights[0][i] = blk.attention1.attention.attention_weights\n",
    "            self._attention_weights[1][i] = blk.attention2.attention.attention_weights\n",
    "        return self.dense(X), state\n",
    "    \n",
    "    def attention_weights(self):\n",
    "        return self._attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e5d10d-1afe-4165-a929-8ffcf1154ea4",
   "metadata": {},
   "source": [
    "### AddNorm\n",
    "encapsulates the common pattern of combining element-wise addition with layer normalization and dropout, which is widely used in transformer architectures to facilitate effective training and improved performance.\n",
    "#### class EncoderBlock & DecoderBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8ce5127b-0160-4a21-8764-5090cb3e5874",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNorm(nn.Module):\n",
    "    \"\"\"The residual connection followed by layer normalization.\"\"\"\n",
    "    def __init__(self, norm_shape, dropout):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln = nn.LayerNorm(norm_shape)\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        return self.ln(self.dropout(Y) + X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c221780c-8528-48fc-b972-d82d02701ed1",
   "metadata": {},
   "source": [
    "### Accumulator\n",
    "\n",
    "#### Used in training of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0899de4b-9188-493a-8c18-ae3f02a0d8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator:\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b438929b-a67b-4281-8ebd-c194664ed9f9",
   "metadata": {},
   "source": [
    "### Masked Softmax CE Loss\n",
    "#### Loss function in training of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "81fb0021-4ed1-437c-ae95-33f0800675ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n",
    "    # `pred` shape: (`batch_size`, `num_steps`, `vocab_size`)\n",
    "    # `label` shape: (`batch_size`, `num_steps`)\n",
    "    # `valid_len` shape: (`batch_size`,)\n",
    "    def forward(self, pred, label, valid_len):\n",
    "        weights = torch.ones_like(label)\n",
    "        weights = sequence_mask(weights, valid_len)\n",
    "        self.reduction='none'\n",
    "        unweighted_loss = super(MaskedSoftmaxCELoss, self).forward(pred.permute(0, 2, 1), label)\n",
    "        weighted_loss = (unweighted_loss * weights).mean(dim=1)\n",
    "        return weighted_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5775cb-71a6-429b-ba2f-6b892d253951",
   "metadata": {},
   "source": [
    "### Transformer\n",
    "#### Initialize full model (encoder+decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3771a77f-39a2-45fc-8047-a5196ea95fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, enc_X, dec_X, *args):\n",
    "        enc_all_outputs = self.encoder(enc_X, *args)\n",
    "        dec_state = self.decoder.init_state(enc_all_outputs, *args)\n",
    "        # Return decoder output only\n",
    "        return self.decoder(dec_X, dec_state)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaefc4e",
   "metadata": {},
   "source": [
    "Train and predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "df86505a-9013-408e-bb0e-2584a291e1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device, starttime):    \n",
    "    net.to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    loss = MaskedSoftmaxCELoss()\n",
    "    net.train()\n",
    "    train_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        metric = Accumulator(2)  # Sum of training loss, no. of tokens\n",
    "        for batch in data_iter:\n",
    "            optimizer.zero_grad()\n",
    "            X, X_valid_len, Y, Y_valid_len = [x.to(device) for x in batch]\n",
    "            bos = torch.tensor([tgt_vocab['<bos>']] * Y.shape[0],device=device).reshape(-1, 1)\n",
    "            dec_input = torch.cat([bos, Y[:, :-1]], 1)  # Teacher forcing\n",
    "            Y_hat = net(X, dec_input, X_valid_len)\n",
    "            l = loss(Y_hat, Y, Y_valid_len)\n",
    "            l.sum().backward()  # Make the loss scalar for `backward`\n",
    "            grad_clipping(net, 1)\n",
    "            num_tokens = Y_valid_len.sum()\n",
    "            optimizer.step()\n",
    "            train_losses.append(l)\n",
    "            with torch.no_grad():\n",
    "                metric.add(l.sum(), num_tokens)\n",
    "        print(f\"Done with epoch number: {epoch+1}\") # optional step\n",
    "        print(f'Verstreken tijd: {(perf_counter() - starttime) / 60.0:.1f} minuten.')\n",
    "    print(f'loss {metric[0] / metric[1]:.3f} on {str(device)}')\n",
    "    return train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e8127f64-afe3-43c3-879a-7c5b8a9383f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_seq2seq(net, src_sentence, src_vocab, tgt_vocab, num_steps,device, save_attention_weights=False):\n",
    "    # Set `net` to eval mode for inference\n",
    "    net.eval()\n",
    "    src_tokens = src_vocab[src_sentence.lower().split(' ')] + [src_vocab['<eos>']]\n",
    "    enc_valid_len = torch.tensor([len(src_tokens)], device=device)\n",
    "    src_tokens = truncate_pad(src_tokens, num_steps, src_vocab['<pad>'])\n",
    "    src_tokens = [x for x in src_tokens if str(x).isdigit()]\n",
    "    # Unsqueeze adds another dimension that works as the the batch axis here\n",
    "    enc_X = torch.unsqueeze(torch.tensor(src_tokens, dtype=torch.long, device=device), dim=0)\n",
    "    enc_outputs = net.encoder(enc_X, enc_valid_len)\n",
    "    dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)\n",
    "    # Add the batch axis to the decoder now\n",
    "    dec_X = torch.unsqueeze(torch.tensor([tgt_vocab['<bos>']], dtype=torch.long, device=device), dim=0)\n",
    "    output_seq, attention_weight_seq = [], []\n",
    "    for _ in range(num_steps):\n",
    "        Y = net.decoder(dec_X, dec_state)[0]\n",
    "        # We use the token with the highest prediction likelihood as the input\n",
    "        # of the decoder at the next time step\n",
    "        dec_X = Y.argmax(dim=2)\n",
    "        pred = dec_X.squeeze(dim=0).type(torch.int32).item()\n",
    "        # Save attention weights\n",
    "        if save_attention_weights:\n",
    "            attention_weight_seq.append(net.decoder.attention_weights)\n",
    "            # Once the end-of-sequence token is predicted, the generation of the output sequence is complete\n",
    "        if pred == tgt_vocab['<eos>']:\n",
    "                break\n",
    "        output_seq.append(pred)\n",
    "    if len(output_seq)<2:\n",
    "\n",
    "        if len(output_seq)==1: \n",
    "            return ''.join(tgt_vocab.to_tokens(output_seq[0])), attention_weight_seq   \n",
    "        else:\n",
    "\n",
    "            return \"No output!\", attention_weight_seq\n",
    "    else:\n",
    "        return ' '.join(tgt_vocab.to_tokens(output_seq)), attention_weight_seq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880123ee-c29e-41de-ba5e-89bffc0d7c38",
   "metadata": {},
   "source": [
    "### Running model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2818367f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "# Load the ROUGE metric\n",
    "import evaluate\n",
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7e355b7f-c68c-4c53-836f-f4aaa7cc8937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with epoch number: 1\n",
      "Verstreken tijd: 0.0 minuten.\n",
      "Done with epoch number: 2\n",
      "Verstreken tijd: 0.1 minuten.\n",
      "Done with epoch number: 3\n",
      "Verstreken tijd: 0.1 minuten.\n",
      "Done with epoch number: 4\n",
      "Verstreken tijd: 0.2 minuten.\n",
      "Done with epoch number: 5\n",
      "Verstreken tijd: 0.2 minuten.\n",
      "Done with epoch number: 6\n",
      "Verstreken tijd: 0.2 minuten.\n",
      "Done with epoch number: 7\n",
      "Verstreken tijd: 0.3 minuten.\n",
      "Done with epoch number: 8\n",
      "Verstreken tijd: 0.3 minuten.\n",
      "Done with epoch number: 9\n",
      "Verstreken tijd: 0.4 minuten.\n",
      "Done with epoch number: 10\n",
      "Verstreken tijd: 0.4 minuten.\n",
      "loss 0.011 on cpu\n",
      "Verstreken tijd: 0.4 minuten.\n",
      "SAMPLE : Current soil water status is nearing the full point. Plant damage and cost inefficiency can occur if the full point is exceeded. Currently only 4.08 mm of storage capacity remain for additional irrigation and rain. An additional irrigation at this time could add input costs with zero or negative agronomic benefit. In situations with low root zone water storage capacity, it can be effective to apply frequent smaller irrigations\n",
      "ACTUAL : The soil water status is nearing the full point, risking plant damage and cost inefficiency if exceeded. Only 4.08 mm of storage capacity remain.\n",
      "PREDICTED : Soil water is nearing full capacity, risking plant damage and cost inefficiency if exceeded. Only 0.45 mm of storage capacity remains, and additional irrigation now could incur costs with minimal agronomic benefit.\n",
      "\n",
      "SAMPLE : Soil water is now less than the refill point. Plant water stress can occur if refill status persists. At least 9.12 mm of additional soil water is needed to achieve optimal soil water status, and there is currently storage capacity for up to 30.45 mm of additional irrigation and rainwater. In some crops, intentional crop drying practices are sometimes needed. Dry soil conditions may be desirable in those certain situations. The circumstances are favorable for a well-protected crop.\n",
      "ACTUAL : Soil water is below refill point, risking plant water stress. An additional 9.12 mm of water is needed to reach optimal soil moisture levels, with capacity for up to 30.45 mm. The circumstances are favorable for a well protected crop.\n",
      "PREDICTED : Soil water is below refill point, risking plant water stress. An additional 0.78 mm of water is needed to reach optimal soil moisture levels, with capacity for up to reach optimal soil moisture levels, with\n",
      "\n",
      "SAMPLE : Soil water is now less than the refill point. Plant water stress can occur if refill status persists. At least 9.89 mm of additional soil water is needed to achieve optimal soil water status, and there is currently storage capacity for up to 28.91 mm of additional irrigation and rain water. In some crops, intentional crop drying practices are sometimes needed. Dry soil conditions may be desirable in those certain situations. The circumstances are favorable for a well protected crop.\n",
      "ACTUAL : Soil water is below refill point, risking plant water stress. An additional 9.89 mm of water is needed to reach optimal soil moisture levels, with capacity for up to 28.91 mm. The circumstances are favorable for a well protected crop.\n",
      "PREDICTED : Soil water is below refill point, risking plant water stress. An additional 9.89 mm of water is needed to reach optimal soil moisture levels, with capacity for up to reach optimal soil moisture levels, with\n",
      "\n",
      "SAMPLE : Soil water is now less than the refill point. Plant water stress can occur if refill status persists. At least 9.89 mm of additional soil water is needed to achieve optimal soil water status, and there is currently storage capacity for up to 28.91 mm of additional irrigation and rain water. In some crops, intentional crop drying practices are sometimes needed. Dry soil conditions may be desirable in those certain situations. The circumstances are favorable for a well protected crop.\n",
      "ACTUAL : Soil water is below refill point, risking plant water stress. An additional 9.89 mm of water is needed to reach optimal soil moisture levels, with capacity for up to 28.91 mm. The circumstances are favorable for a well protected crop.\n",
      "PREDICTED : Soil water is below refill point, risking plant water stress. An additional 9.89 mm of water is needed to reach optimal soil moisture levels, with capacity for up to reach optimal soil moisture levels, with\n",
      "\n",
      "SAMPLE : Current soil water status is nearing the full point. Plant damage and cost inefficiency can occur if the full point is exceeded. Currently only 1.67 mm of storage capacity remain for additional irrigation and rain. An additional irrigation at this time could add input costs with zero or negative agronomic benefit. In situations with low root zone water storage capacity, it can be effective to apply frequent smaller irrigations.\n",
      "ACTUAL : Soil water is nearing full capacity, risking plant damage and cost inefficiency if exceeded. Only 1.67 mm of storage capacity remains, and additional irrigation now could incur costs with minimal agronomic benefit.\n",
      "PREDICTED : Soil water is nearing full capacity, risking plant damage and cost inefficiency if exceeded. Only 0.45 mm of storage capacity remains, and additional irrigation now could incur costs with minimal agronomic benefit.\n",
      "\n",
      "SAMPLE : Soil water is currently in the optimal range for plant health. There is currently storage capacity for up to 13.5 mm of additional irrigation and rain water. Soil water is not expected to fall below the refill point within the next 7 days, even without irrigation. With current conditions, optimal status can be maintained by irrigating now or by waiting a little longer.\n",
      "ACTUAL : Soil water is at optimal levels for plant health, with capacity for 13.5 mm of additional water. Irrigating within the next 7 days will help maintain this optimal range.  \n",
      "PREDICTED : Soil water is at optimal levels for plant health, with capacity for 12.84 mm of additional water. Irrigating within the next 7 days will help maintain this optimal range.\n",
      "\n",
      "SAMPLE : Soil water is now less than the refill point. Plant water stress can occur if refill status persists. At least 8.67 mm of additional soil water is needed to achieve optimal soil water status, and there is currently storage capacity for up to 25.09 mm of additional irrigation and rainwater. In some crops, intentional crop drying practices are sometimes needed. Dry soil conditions may be desirable in those certain situations. The circumstances are favorable for a well-protected crop.\n",
      "ACTUAL : Soil water is below refill point, risking plant water stress. An additional 8.67 mm of water is needed to reach optimal soil moisture levels, with capacity for up to 25.09 mm. The circumstances are favorable for a well protected crop.\n",
      "PREDICTED : Soil water is below refill point, risking plant water stress. An additional 9.89 mm of water is needed to reach optimal soil moisture levels, with capacity for up to reach optimal soil moisture levels, with\n",
      "\n",
      "SAMPLE : Soil water is currently in the optimal range for plant health. There is currently storage capacity for up to 13.5 mm of additional irrigation and rain water. Soil water is not expected to fall below the refill point within the next 7 days, even without irrigation. With current conditions, optimal status can be maintained by irrigating now or by waiting a little longer.\n",
      "ACTUAL : Soil water is at optimal levels for plant health, with capacity for 13.5 mm of additional water. Irrigating within the next 7 days will help maintain this optimal range.  \n",
      "PREDICTED : Soil water is at optimal levels for plant health, with capacity for 12.84 mm of additional water. Irrigating within the next 7 days will help maintain this optimal range.\n",
      "\n",
      "SAMPLE : Soil water is now less than the refill point. Plant water stress can occur if refill status persists. At least 0.78 mm of additional soil water is needed to achieve optimal soil water status, and there is currently storage capacity for up to 28.41 mm of additional irrigation and rain water. In some crops, intentional crop drying practices are sometimes needed. Dry soil conditions may be desirable in those certain situations. Immediatie action by application of contact fungicide is required for Powdery Mildew. The chance that the crop will be at risk soon is high. We strongly recommend to apply contact fungicide to prevent a likely infection. Consider application of systemic fungicide for Grape downy Mildew. Your crop might be infected with Plasmopara viticola, consider applying a systemic fungicide to suppress a recent infection from within the plant, to prevent further damage and to prevent a new infection source from within your field.\n",
      "ACTUAL : Soil water is below refill point, risking plant water stress. An additional 0.78  mm of water is needed to reach optimal soil moisture levels, with capacity for up to 28.41 mm. The chance that the crop will be at risk soon is high, immediate action is advised to prevent likely infections. \n",
      "PREDICTED : Soil water is below refill point, risking plant water stress. An additional 0.78 mm of water is needed to reach optimal soil moisture levels, with capacity for up to reach optimal soil moisture levels, with\n",
      "\n",
      "SAMPLE : Soil water is now less than the refill point. Plant water stress can occur if refill status persists. At least 11.89 mm of additional soil water is needed to achieve optimal soil water status, and there is currently storage capacity for up to 39.56 mm of additional irrigation and rainwater. In some crops, intentional crop drying practices are sometimes needed. Dry soil conditions may be desirable in those certain situations. The circumstances are favorable for a well-protected crop.\n",
      "ACTUAL : Soil water is below refill point, risking plant water stress. An additional 11.89 mm of water is needed to reach optimal soil moisture levels, with capacity for up to 39.56 mm. The circumstances are favorable for a well protected crop.\n",
      "PREDICTED : Soil water is below refill point, risking plant water stress. An additional 9.89 mm of water is needed to reach optimal soil moisture levels, with capacity for up to reach optimal soil moisture levels, with\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tokenize\n",
    "src_tokens = tokenize(x_train)\n",
    "tgt_tokens = tokenize(y_train)\n",
    "\n",
    "# build vocabulary on dataset\n",
    "src_vocab = Vocab(src_tokens, reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "tgt_vocab = Vocab(tgt_tokens, reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "\n",
    "max_len_text=100 \n",
    "max_len_summary=35\n",
    "\n",
    "src_array, src_valid_len = build_array_sum(src_tokens, src_vocab, max_len_text)\n",
    "tgt_array, tgt_valid_len = build_array_sum(tgt_tokens, tgt_vocab, max_len_summary)\n",
    "data_arrays = (src_array, src_valid_len, tgt_array, tgt_valid_len)\n",
    "\n",
    "#Create tensor dataset object\n",
    "batch_size = 5\n",
    "data_iter = load_array(data_arrays, batch_size)\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "# Initialize model\n",
    "num_hiddens, num_layers, dropout, num_steps = 32, 2, 0.1, 10\n",
    "ffn_num_input, ffn_num_hiddens, num_heads = 32, 64, 4\n",
    "key_size, query_size, value_size = 32, 32, 32\n",
    "norm_shape = [32]\n",
    "encoder = TransformerEncoder(len(src_vocab), key_size, query_size, value_size, num_hiddens,norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,num_layers, dropout)\n",
    "decoder = TransformerDecoder(len(tgt_vocab), key_size, query_size, value_size, num_hiddens,norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,num_layers, dropout)\n",
    "\n",
    "net = Transformer(encoder, decoder)\n",
    "# nn.init.xavier_uniform_(net.weight) # for initialising the weights of the fully connected layers in the model\n",
    "\n",
    "\n",
    "from time import perf_counter\n",
    "starttime = perf_counter()\n",
    "lr = 0.005\n",
    "num_epochs = 10\n",
    "train_losses = train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device, starttime)\n",
    "print(f'Verstreken tijd: {(perf_counter() - starttime) / 60.0:.1f} minuten.')\n",
    "\n",
    "sample = x_test[:10]\n",
    "actual = y_test[:10]\n",
    "predictions = []\n",
    "for s, a in zip(sample, actual):\n",
    "    pred_sum, _ = predict_seq2seq(net, s, src_vocab, tgt_vocab, 35, device)\n",
    "    predictions.append(pred_sum)\n",
    "    print(\"SAMPLE : {}\".format(s))\n",
    "    print(\"ACTUAL : {}\".format(a))\n",
    "    print(\"PREDICTED : {}\".format(pred_sum))\n",
    "    print('')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "feade855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.7802579817186559, 'rouge2': 0.7433754105090313, 'rougeL': 0.7803021181324165, 'rougeLsum': 0.7817048468772607}\n"
     ]
    }
   ],
   "source": [
    "y = actual.tolist()\n",
    "\n",
    "ref = []\n",
    "for i in y:\n",
    "    ref.append([i])\n",
    "\n",
    "results = rouge.compute(predictions=predictions, references=ref)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f452200",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
